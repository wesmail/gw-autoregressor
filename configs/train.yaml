# Training config for GPT (time-domain only) on seismic waveforms
# Run: python main.py fit --config configs/train.yaml
# Data must provide x, y (time tokens and target).

seed_everything: 42
trainer:
  max_epochs: 10
  accelerator: "gpu"
  devices: "-1"
  strategy: "auto"
  precision: "bf16-mixed" # bf16-mixed or 32-true
  accumulate_grad_batches: 2
  #gradient_clip_val: 1.0
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        save_weights_only: false
        mode: "min"
        monitor: "val_loss"
        every_n_train_steps: 0
        every_n_epochs: 1
        train_time_interval: null
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: "val_loss"
        min_delta: 0.0
        patience: 5
        verbose: false
  logger:
    - class_path: lightning.pytorch.loggers.TensorBoardLogger
      init_args:
        save_dir: "."
        name: "logs"   

model:
  class_path: models.lightning_module.GPTLightning
  init_args:
    # GPT architecture (must match data kernel_size)
    in_channels: 2
    kernel_size: 64
    d_model: 128
    num_heads: 8
    num_enc_layers: 12
    dropout: 0.1
    max_len: 5000
    dim_feedforward_multiplier: 4
    # Token causal CNN embedding
    token_cnn_kernel: 7
    token_cnn_layers: 4
    token_cnn_dilation_growth: 2
    token_cnn_dropout: 0.0
    fusion_type: "concat"
    freq_embed_type: "mlp"    # "mlp" (lightweight) or "conv" (legacy)
    freq_keep_bins: 4
    freq_norm: "mean"         # must match data; "none" | "mean" | "l2"
    freq_log1p: true
    # Post-head stitcher
    use_stitcher: true
    stitcher_hidden: 64
    stitcher_kernel: 5
    stitcher_layers: 2
    stitcher_dropout: 0.0
    # Loss
    time_loss: "mse"
    # conditioning
    theta_dim: 4
    cond_dim: 128
    cond_hidden: 256
    lr: 1.0e-4
    # Optional LR scheduler (omit or set scheduler: null to disable)
    scheduler: CosineAnnealingWarmRestarts
    scheduler_T_0: 5
    scheduler_T_mult: 2
    scheduler_eta_min: 1.0e-5
    lr_scheduler_interval: epoch
    lr_scheduler_frequency: 1
    lr_scheduler_monitor: null

    # =========================================================
    # Scheduled Sampling (set ss_enabled: true to activate)
    # =========================================================
    ss_enabled: false             # master switch — false preserves original behavior
    ss_warmup_steps: 2000         # linearly anneal p from p_start → p_end over this many steps
    ss_p_start: 0.0               # initial probability of feeding back own prediction
    ss_p_end: 0.2                 # final probability
    ss_unroll_steps: 20           # number of autoregressive steps per training batch (2–8 rec.)
    ss_detach_pred: true          # detach predicted tokens when feeding back (stable/minimal)
    ss_use_cache: false           # use KV-cache during unroll (optional speedup, experimental)
    ss_focus_fraction: 0.7        # 1.0 = uniform start; <1.0 = sample from last X% (e.g. merger)

data:
  class_path: datasets.data_handling.GWDataModule
  init_args:
    data_dir: /mnt/d/waleed/G2Net/ET/EinsteinTelescope-WaveSim/data
    kernel_size: 64
    stride: 64
    n_files: 4
    n_samples_per_file: 10000
    batch_size: 16
    num_workers: 16
    freq_keep_bins: 4
    freq_log1p: true
    freq_norm: "mean"   # "none" | "mean" | "l2"
    max_samples: 57500  # max time samples per waveform (null = no cap)